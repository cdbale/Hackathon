---
title: "Untitled"
author: "Cameron Bale"
date: "8/29/2020"
output: html_document
---

Install and load packages.
```{r}
#install.packages(c('plyr', 'tidyverse', 'sp', 'patchwork', 'spatstat', 'ggmap', 'geosphere'))
library(plyr)
library(tidyverse)
library(sp)
library(patchwork)
library(spatstat)
library(ggmap)
library(geosphere)
```

Load cleaned data.
```{r}
load(file = "../Data/korea_data_clean.RData")
options(digits = 10)
```

View latitude and longitude tuples.
```{r}
full %>% select(latitude, longitude)
```

How many individuals in the data?
```{r}
full %>%
  distinct(patient_id) %>%
  summarize(n = n())
```

What time frame does the data cover?
```{r}
full %>%
  summarize(min_date = min(date),
            max_date = max(date))
```

What are the minimum and maximum trajectory lengths?
```{r}
full %>%
  group_by(patient_id) %>%
  summarize(n = n(), .groups = 'drop') %>%
  summarize(min_length = min(n),
            max_length = max(n))
```

Distribution of trajectory lengths.
```{r}
full %>%
  group_by(patient_id) %>%
  summarize(n = n(), .groups = 'keep') %>%
  ggplot(aes(x = n)) +
  geom_histogram(binwidth = 1) +
  labs(title = 'Trajectory Length Distribution',
       y = 'Number of Individuals',
       x = "Number of Location Tuples in Trajectories") +
  theme(text = element_text(size = 12.5))
```

Create data for full korea data and seoul data only for individuals with at least five observed location points. We could have taken the observations with `province` == Seoul, but for the purposes of our analysis we wanted an area with higher location density. We take points in a rectangular region roughly centered on Seoul.
```{r}
full <- full %>%
  select(patient_id, latitude, longitude) %>%
  group_by(patient_id) %>%
  filter(n() > 4) %>%
  ungroup()

s_full <- full %>%
  filter((longitude > 126.85 & longitude < 127.15) & (latitude > 37.45 & latitude < 37.65)) %>%
  group_by(patient_id) %>%
  filter(n() > 4) %>%
  ungroup()
```

How many individuals after restriction.
```{r}
full %>%
  distinct(patient_id) %>%
  summarize(n = n())

s_full %>%
  distinct(patient_id) %>%
  summarize(n = n())
```

How many distinct location tuples, and what is the percentage of them are unique to one individual?
```{r}
(np <- full %>%
  distinct(latitude, longitude) %>%
  summarize(n = n()) %>%
  pull())

nup <- full %>%
  group_by(latitude, longitude) %>%
  summarize(n = n(), .groups = 'drop') %>%
  filter(n == 1) %>%
  summarize(n = n()) %>%
  pull()

nup/np * 100
```

What percentage of individuals have a unique location trajectory?
```{r}
split_trajs <- full %>%
  group_by(patient_id) %>%
  group_split(.keep = FALSE)

split_trajs <- lapply(split_trajs, function(x) arrange(x, latitude))

length(unique(split_trajs))/length(split_trajs) * 100
```

Calculate the density of points per square kilometer in the full dataset, excluding 16 points on an island South of the mainland (below 34 degrees latitude).
```{r}
# original data is in latitude and longitude
# have to project it onto a 2D space in terms of meters and divide by 1000 to get it in kilometers
# https://proj.org/operations/projections/utm.html
# it is a universal transverse mercator projection using the sp package
f <- full %>%
  select(longitude, latitude) %>%
  filter(latitude > 34)

coordinates(f) <- c("longitude", "latitude")
proj4string(f) <- CRS("+proj=longlat +datum=WGS84")
fm <- spTransform(f, CRS("+proj=utm +zone=52"))
fm <- as.data.frame(fm)/1000

# define window to calculate intensity in by the bounds created by the minimum and maximum observed latitude and longitude values
window <- data.frame(
  min_long = min(fm$longitude),
  min_lat = min(fm$latitude),
  max_long = max(fm$longitude),
  max_lat = max(fm$latitude)
)

# create ppp object to pass to intensity function
f_ppp <- ppp(fm$longitude, fm$latitude, c(window$min_long, window$max_long), c(window$min_lat, window$max_lat))

intensity(f_ppp)
```

Calculate the density of points per square kilometer in Seoul.
```{r}
s <- s_full %>%
  select(longitude, latitude)

coordinates(s) <- c("longitude", "latitude")
proj4string(s) <- CRS("+proj=longlat +datum=WGS84")
sm <- spTransform(s, CRS("+proj=utm +zone=52"))
sm <- as.data.frame(sm)/1000

s_window <- data.frame(
  min_long = min(sm$longitude),
  min_lat = min(sm$latitude),
  max_long = max(sm$longitude),
  max_lat = max(sm$latitude)
)

s_ppp <- ppp(sm$longitude, sm$latitude, c(s_window$min_long, s_window$max_long), c(s_window$min_lat, s_window$max_lat))

intensity(s_ppp)
```

Output maps of the full dataset and Seoul.
```{r fig.width = 10, fig.height = 8}
k_map <- ggmap(get_map(location = c(126, 33, 130, 38.5)), 
               extent = 'panel', 
               base_layer = ggplot(full, aes(x = longitude, y = latitude))) +
  geom_point(alpha = 0.4, color = 'blue') +
  labs(x = 'Longitude',
       y = 'Latitude')

print(k_map)
```

```{r}
s_map <- ggmap(get_map(location = c(126.85, 37.45, 127.15, 37.65)), 
               extent = 'panel', 
               base_layer = ggplot(s_full, aes(x = longitude, y = latitude))) +
  geom_point(alpha = 0.4, color = 'blue') +
  labs(x = '',
       y = '')

print(s_map)
```

Define a custom rounding function. Numbers ending in 1 - 4 get rounded down, numbers ending in 5 - 9 get rounded up.
```{r}
round2 = function(x, n) {
  rx = abs(x)*10^n
  rx = rx + 0.5 + sqrt(.Machine$double.eps)
  rx = trunc(rx)
  rx = rx/10^n
  return(rx)
}
```

Create maps showing coarsened trajectories.
```{r fig.width = 10, fig.height = 8}
traj_1d <- full %>%
  mutate(latitude = round2(latitude, n = 1),
         longitude = round2(longitude, n = 1))

coarse_k_map <- ggmap(get_map(location = c(126, 33, 130, 38.5)), 
               extent = 'panel', 
               base_layer = ggplot(traj_1d, aes(x = longitude, y = latitude))) +
  geom_point(alpha = 0.4, color = 'blue') +
  labs(x = 'Longitude',
       y = 'Latitude')

print(coarse_k_map)
```

```{r}
traj_1ds <- s_full %>%
  mutate(latitude = round2(latitude, n = 1),
         longitude = round2(longitude, n = 1))

coarse_s_map <- ggmap(get_map(location = c(126.85, 37.45, 127.15, 37.65)), 
               extent = 'panel', 
               base_layer = ggplot(traj_1ds, aes(x = longitude, y = latitude))) +
  geom_point(color = 'blue') +
  labs(x = '',
       y = '')

print(coarse_s_map)
```

Calculating distance between points for individual i (From paper).
```{r}
distGeo(c(127.0170, 37.59256), c(127.017, 37.593))
```

### Singling-out: Location Coarsening

Define `trajectory_uniqueness` function. Determines the percentage of unique trajectories in a simulated database. The simulated database is created by randomly sampling `num_points` points from each trajectory in `trajectory_data`.
```{r}
trajectory_uniqueness <- function(trajectory_data, num_points) {
  
  # calculate the total number of trajectories in the data
  num_trajectories <- length(unique(trajectory_data$patient_id))
  
  # randomly sample 'num_points' points from each trajectory
  # split into list - each element is a sampled trajectory (patient_id is dropped)
  sampled_data <- trajectory_data %>%
    group_by(patient_id) %>%
    slice_sample(n = num_points) %>%
    group_split(.keep = FALSE)
  
  # order each trajectory based on 'latitude' values - necessary for trajectory comparison using 'unique' function
  sampled_data <- lapply(sampled_data, function(x) arrange(x, latitude))
  
  # calculate the percentage of sampled trajectories that are unique
  percent_unique <- 100 * (length(unique(sampled_data))/num_trajectories)
  
  return(as_tibble(percent_unique))
  
}
```

Define `uniqueness_sims` function. Use this function to perform multiple simulations for different numbers of sampled trajectory points (`sample_size`) for a given number of decimals at which locations are measured (`num_dec`).
```{r}
uniqueness_sims <- function(location_data, num_dec, nits = 100, sample_size = 1:5, pvar_names = c('one', 'two', 'three', 'four', 'five')) {
  
  # round latitude and longitude to the specified number of decimals
  loc_data <- location_data %>%
    mutate_at(.vars = c('latitude', 'longitude'), round2, n = num_dec)
  
  # calculate the percentage of unique trajectories in 'nits' simulated databases for each 'sample_size'
  unique_pcts <- map_dfc(sample_size, function(y) map_dfr(1:nits, function(x) trajectory_uniqueness(loc_data, y)))
  
  # assign column names to mark the sample size
  colnames(unique_pcts) <- pvar_names
  
  # transform into long format - one column for the number of sampled points, one for the percentage of unique trajectories, 
  # and add a column for the number of decimals
  # each row is for one simulated database, i.e. unique_res[0,] = (number_of_sampled_points, percent_unique_trajectories, number of decimals)
  unique_res <- unique_pcts %>%
    gather(key = 'num_points', value = 'percent_unique') %>%
    mutate(specificity = num_dec)
  
  return(unique_res)
  
}
```

Perform simulation 100 times for sample sizes of one to five points, for locations measured from five to zero decimals.
```{r}
uniqueness_results_full <- uniqueness_sims(full, 5) %>%
  bind_rows(uniqueness_sims(full, 4)) %>%
  bind_rows(uniqueness_sims(full, 3)) %>%
  bind_rows(uniqueness_sims(full, 2)) %>%
  bind_rows(uniqueness_sims(full, 1)) %>%
  bind_rows(uniqueness_sims(full, 0))
```

The simulation takes a long time to run, so save the results.
```{r}
save(uniqueness_results_full, file = '../Data/uniqueness_results_full.RData')
```

Load uniqueness calculations for full data (can do this after running the above chunks once).
```{r}
load('../Data/uniqueness_results_full.RData')
```

Repeat for Seoul.
```{r}
uniqueness_results_seoul <- uniqueness_sims(s_full, 5) %>%
  bind_rows(uniqueness_sims(s_full, 4)) %>%
  bind_rows(uniqueness_sims(s_full, 3)) %>%
  bind_rows(uniqueness_sims(s_full, 2)) %>%
  bind_rows(uniqueness_sims(s_full, 1)) %>%
  bind_rows(uniqueness_sims(s_full, 0))
```

The simulation takes a long time to run, so save the results.
```{r}
save(uniqueness_results_seoul, file = '../Data/uniqueness_results_seoul.RData')
```

Load uniqueness calculations for seoul data (can do this after running the above chunk once).
```{r}
load('../Data/uniqueness_results_seoul.Rdata')
```

Plot uniqueness results for South Korea and Seoul side by side.
```{r fig.width = 10, fig.height = 8}
boxes_full <- uniqueness_results_full %>%
  mutate(specificity = factor(specificity, levels = c('0', '1', '2', '3', '4', '5')),
         num_points = factor(num_points, levels = c('one', 'two', 'three', 'four', 'five'))) %>%
  ggplot(aes(x = specificity, y = percent_unique, fill = factor(num_points))) +
  geom_boxplot(position = position_dodge(width = 0)) +
  labs(x = 'Number of Decimals',
       y = '% Unique Trajectories',
       fill = '# Tuples\n',
       title = 'Full Data') +
  scale_fill_discrete(labels = c('One', 'Two', 'Three', 'Four', 'Five'),
                      guide = guide_legend(reverse = TRUE)) +
  theme(legend.position = c(.9, .1),
        legend.justification = c('right', 'bottom'),
        legend.box.just = 'right',
        text = element_text(size = 12.5)) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, by = 25))

boxes_s <- uniqueness_results_seoul %>%
  mutate(specificity = factor(specificity, levels = c('0', '1', '2', '3', '4', '5')),
         num_points = factor(num_points, levels = c('one', 'two', 'three', 'four', 'five'))) %>%
  ggplot(aes(x = specificity, y = percent_unique, fill = factor(num_points))) +
  geom_boxplot(position = position_dodge(width = 0)) +
  labs(x = 'Number of Decimals',
       y = '',
       fill = '# Tuples\n',
       title = 'Seoul Only') +
  scale_fill_discrete(labels = c('One', 'Two', 'Three', 'Four', 'Five'),
                      guide = guide_legend(reverse = TRUE)) +
  theme(legend.position = c(.9, .1),
        legend.justification = c('right', 'bottom'),
        legend.box.just = 'right',
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        text = element_text(size = 12.5)) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, by = 25))

addSmallLegend <- function(myPlot, pointSize = 2, textSize = 10, spaceLegend = 1) {
    myPlot +
        guides(shape = guide_legend(override.aes = list(size = pointSize)),
               color = guide_legend(override.aes = list(size = pointSize))) +
        theme(legend.title = element_text(size = textSize), 
              legend.text  = element_text(size = textSize),
              legend.key.size = unit(spaceLegend, "lines"))
}

addSmallLegend(boxes_full) | addSmallLegend(boxes_s)
```

Calculate the average percentage of unique trajectories across all simulations at each value of d for the full data.
```{r}
uniqueness_results_full %>% group_by(specificity) %>% summarize(avg_pct_unique = mean(percent_unique), .groups = 'drop')
```

Calculate the average percentage of unique trajectories across all simulations at each value of d for the Seoul data.
```{r}
uniqueness_results_seoul %>% group_by(specificity) %>% summarize(avg_pct_unique = mean(percent_unique), .groups = 'drop')
```

Calculate the distribution of distances that points are shifted by location coarsening.
```{r}
distances_shifted <- function(location_data, num_dec) {
  
  ds <- location_data %>%
    mutate(latitude2 = round2(latitude, n = num_dec),
           longitude2 = round2(longitude, n = num_dec),
           distances = distGeo(tibble(longitude, latitude), tibble(longitude2, latitude2))) %>%
    select(distances)
  
  return(ds)
  
}
```

Calculate distances shifted for location tuples measured from zero to four decimals.
```{r}
all_distances <- map_dfc(0:5, function(x) distances_shifted(full, x))
```

We also make a function to display the quantiles of the distances that observed location points were shifted when we round `latitude` and `longitude`.
```{r}
dist_quants <- function(distances){
  
  qs <- tibble(
    Min = min(distances),
    `2.5%` = quantile(distances, probs = .025),
    `50%` = median(distances),
    `97.5%` = quantile(distances, probs = 0.975),
    Max = max(distances)
  )
  
  return(qs)
  
}
```

Calculate the quantiles of the distributions of distances shifted for locations measured from zero to four decimals.
```{r}
map_dfr(all_distances, function(x) dist_quants(x)) %>%
  mutate(Specificity = 0:5) %>%
  mutate_at(.vars = c('Min', '2.5%', '50%', '97.5%', 'Max'), round2, n = 2)
```

### Singling-out: Aggregated Counts

No simulation here - just the percent and number of regions containing at least two location tuples.
```{r}
bin_dist <- function(loc_data, num_dec) {
  
  # obtain the number of times each location tuple occurs - each tuple marks the center of a region - number of occurrences of a tuple is the number of points originally in that region. Select the variable with the number of occurences for each tuple, and create a variable 'd' to indicate the number of decimals
  loc_data %>%
    mutate(latitude = round2(latitude, num_dec),
           longitude = round2(longitude, num_dec)) %>%
    group_by(latitude, longitude) %>% 
    summarize(n = n(), .groups = 'drop') %>%
    select(n) %>%                           
    mutate(d = factor(num_dec))
  
}

# perform calculation on full data for values of d from 5 to 0
bin_dists <- bin_dist(full, num_dec = 5) %>%
  bind_rows(bin_dist(full, num_dec = 4),
            bin_dist(full, num_dec = 3),
            bin_dist(full, num_dec = 2),
            bin_dist(full, num_dec = 1),
            bin_dist(full, num_dec = 0))

# create variable indicating regions with more than one location tuple
# calculate the percentage and number of regions with more than one location tuple
bin_dists %>%
  mutate(not_singled_out = if_else(n > 1, 1, 0)) %>%
  group_by(d) %>%
  summarize(pct_nso = mean(not_singled_out) * 100,
            num_nso = sum(not_singled_out), .groups = 'drop')
```

Repeat for Seoul.
```{r}
bin_dists_s <- bin_dist(s_full, num_dec = 5) %>%
  bind_rows(bin_dist(s_full, num_dec = 4),
            bin_dist(s_full, num_dec = 3),
            bin_dist(s_full, num_dec = 2),
            bin_dist(s_full, num_dec = 1),
            bin_dist(s_full, num_dec = 0))

bin_dists_s %>%
  mutate(not_singled_out = if_else(n > 1, 1, 0)) %>%
  group_by(d) %>%
  summarize(pct_nso = mean(not_singled_out) * 100,
            num_nso = sum(not_singled_out), .groups = 'drop')
```

Calculate the areas and side lengths of all regions in the data.
```{r}
# make a function for calculating polygon boundaries and area
# designed to take vectors or dataframe columns of latitude and longitude values. Reports the area over which points would
# have been coarsened to the corresponding lat/long point
# this function is specific to this data i.e. it is assumed that latitude originally had 5 decimals, and longitude had 4
poly_area <- function(loc_data, num_dec) {
  
  # create point data
  pt <- loc_data %>% select(-patient_id) %>% mutate_all(round2, n = num_dec)
  
  # get the minimum latitude/longitude values that correspond to the edges of the polygon
  pt$min_lat <- pt$latitude - as.numeric(paste0(".", str_pad("5", num_dec + 1, pad = "0")))
  pt$min_long <- pt$longitude - as.numeric(paste0(".", str_pad("5", num_dec + 1, pad = "0")))
  
  # get the 'maximum' latitude/longitude values that correspond to the edges of the polygon (going out to 7 decimal places)
  pt$max_lat <- pt$latitude + as.numeric(paste0(".", str_pad(str_pad("4", num_dec + 1, pad = "0"), 7, pad = "9", side = "right")))
  pt$max_long <- pt$longitude + as.numeric(paste0(".", str_pad(str_pad("4", num_dec + 1, pad = "0"), 7, pad = "9", side = "right")))
  
  A <- data.frame(apply(pt[,3:6], 1, function(x) 
  {pol <- rbind(c(x[2], x[1]),
                c(x[2], x[3]),
                c(x[4], x[3]),
                c(x[4], x[1]),
                c(x[2], x[1]))
  areaPolygon(pol)}))
  
  names(A) <- "area"
  
  # vertical (or latitudinal) lengths are equal, only need to take one
  A$vert_length <- distGeo(data.frame(pt$max_long, pt$max_lat), data.frame(pt$max_long, pt$min_lat))
  
  # horizontal (or longitudinal) lengths are slightly different, the northern side will be slightly shorter
  # we take the average of the two
  A$hor_length <- apply(data.frame(distGeo(data.frame(pt$max_long, pt$max_lat), data.frame(pt$min_long, pt$max_lat)),
                                   distGeo(data.frame(pt$max_long, pt$min_lat), data.frame(pt$min_long, pt$min_lat))),
                        1, mean)
  
  return(A)
  
}
```

Function to calculate average side lengths and area for regions.
```{r}
poly_stats <- function(trajectory_data) {
  trajectory_data %>%
  summarize(hor_length = mean(hor_length),
            vert_length = mean(vert_length),
            p_area_m = mean(area),
            p_area_km = mean(area)/1000000)
}
```

Calculate areas and side lengths for regions for values of d = 0 to 5.
```{r}
map_dfr(0:5, function(x) poly_stats(poly_area(full, num_dec = x))) %>%
  mutate(number_decimals = 0:5) %>% 
  mutate_at(.vars = c('hor_length', 'vert_length', 'p_area_km'), round2, n = 2) %>%
  mutate_at(.vars = 'p_area_m', round2, n = 1)
```

### Assessing Inference for Trajectories

Function to simulate an adversary having external information location tuples corresponding to COVID-19 positive individuals in a database. The adversary links these external information tuples to all compatible trajectories in Y. The disease status would then be inferred based on the proportion of individuals with compatible trajectories that are COVID-19 positive. This proportion is used to to calculate the left-hand side (LHS) of the inference condition from the paper.
```{r}
inference_simulation <- function(loc_data, ci_length, prior = 0.01) {

  # number of COVID-19 positive individuals in the database
  pos_status_data <- tibble()
  
  # loop to guarantee we assign some COVID-19 positive individuals in the data
  # simulate 1% of individuals in Y as COVID-19 positive
  while(nrow(pos_status_data) == 0) {
  
    # add covid status to location data
    status_data <- loc_data %>% 
      group_by(patient_id) %>%
      mutate(status = sample(c(0, 1), size = 1, prob = c(.99, .01))) %>%
      ungroup()
    
    # data corresponding to covid positive individuals (while loop condition just needs the number of rows > 0)
    pos_status_data <- status_data %>%
      filter(status == 1)

  }

  # sample 'ci_length' tuples from the trajectory of each COVID-19 positive individual
  # these tuples will represent the external information available to an adversary
  # the sample from each trajectory is a dataframe stored as a list element in 'ext_inf'
  ext_inf <- pos_status_data %>%
    group_by(patient_id) %>%
    sample_n(size = ci_length) %>%
    group_split()

  # remove patient id and status from external information samples
  ext_inf2 <- lapply(ext_inf, function(x) x %>% select(-patient_id, -status))

  # summarize each external information dataframe so that it contains one row for each unique external information tuple
  # in a given sample - add a variable for how many times each unique tuple occurred in the original sample
  ext_inf3 <- lapply(ext_inf2, function(x) x %>% group_by(latitude, longitude) %>% summarize(n = n(), .groups = 'drop'))

  # extract the tuples from the original full database that match the tuples in the external information samples - perform a similar summary on the matches - create one row for each unique tuple on the basis of `patient_id` with the number of times each tuple occurs in that patient's trajectory
  matches <- lapply(ext_inf3, function(y) match_df(status_data, y, on = c('latitude', 'longitude')) %>%
                                          group_by(patient_id, latitude, longitude, status) %>%
                                          summarize(n1 = n(), .groups = 'drop'))

  # merge the variables for patient_id, disease_status, and the number of times a given tuple occurs in the patient trajectory with the external information samples - we can now compare the number of times tuples occur in the external information sample to the number of times they occur in a patient's trajectory
  links <- lapply(1:length(ext_inf3), function(x) left_join(matches[[x]], ext_inf3[[x]], by = c('latitude', 'longitude')))

  # filter for the patients with trajectories that contain the same unique tuples occurring at least as many times as the original external information sample
  links2 <- lapply(1:length(links), function(x) links[[x]] %>% group_by(patient_id) %>% filter(n1 >= n & n() >= nrow(ext_inf3[[x]])) %>% ungroup())

  # reduce to list of dataframes with patient_ids and COVID-19 status corresponding to the trajectories that were linked to each external information sample
  covid_count <- lapply(links2, function(x) distinct(x, patient_id, status))

  # calculate left hand side of the inference condition for each external information sample
  inf_props <- map_dfr(covid_count, function(x) x %>% summarize(cond = mean(status) - prior))

  # average the left hand side of the inference condition across all COVID-19 positive individuals
  avg_cond <- inf_props %>% summarize(avg_cond = mean(cond))
  
  return(avg_cond)
  
}
```

Function to perform simulation one hundred times each for one to five external information tuples based on an input dataset and a specified value of d (num_dec).
```{r}
inf_calc <- function(location_data, num_dec, nits = 100, sample_sizes = 1:5, pvar_names = c('one', 'two', 'three', 'four', 'five')) {
  
  loc_data <- location_data %>%
    mutate_at(.vars = c('latitude', 'longitude'), round2, n = num_dec)
  
  props <- map_dfc(1:5, function(y) map_dfr(1:100, function(x) inference_simulation(loc_data, y)))
  
  colnames(props) <- pvar_names
  
  props_g <- props %>%
    gather(key = 'num_points', value = 'inf_prop') %>%
    mutate(specificity = num_dec)
  
  return(props_g)
  
}
```

Inference analysis for full data.
```{r fig.width=6, fig.height=6}
inf_results <- inf_calc(full, 5) %>%
  bind_rows(inf_calc(full, 4)) %>%
  bind_rows(inf_calc(full, 3)) %>%
  bind_rows(inf_calc(full, 2)) %>%
  bind_rows(inf_calc(full, 1)) %>%
  bind_rows(inf_calc(full, 0))

save(inf_results, file = '../Data/inf_results_trajectories_full.RData')
load('../Data/inf_results_trajectories_full.RData')

i_p <- inf_results %>%
  mutate(specificity = factor(specificity, levels = c('0', '1', '2', '3', '4', '5')),
         num_points = factor(num_points, levels = c('one', 'two', 'three', 'four', 'five'))) %>%
  ggplot(aes(x = specificity, y = inf_prop, fill = factor(num_points))) +
  geom_boxplot(position = position_dodge(width = 0.5)) +
  labs(x = 'Number of decimals d',
       y = 'Average LHS Inference Condition Across Positive Individuals',
       fill = '# EI Tuples\n',
       title = 'Full Data') +
  scale_fill_discrete(labels = c('One', 'Two', 'Three', 'Four', 'Five'),
                      guide = guide_legend(reverse = TRUE)) +
  theme(legend.position = c(0.025, 0.95),
        legend.justification = c('left', 'top'),
        legend.box.just = 'left',
        text = element_text(size = 12.5))

addSmallLegend(i_p)
```

Inference analysis for Seoul data.
```{r fig.width=6, fig.height=6}
inf_results_s <- inf_calc(s_full, 5) %>%
  bind_rows(inf_calc(s_full, 4)) %>%
  bind_rows(inf_calc(s_full, 3)) %>%
  bind_rows(inf_calc(s_full, 2)) %>%
  bind_rows(inf_calc(s_full, 1)) %>%
  bind_rows(inf_calc(s_full, 0))

save(inf_results_s, file = '../Data/inf_results_trajectories_seoul.RData')
load(file = '../Data/inf_results_trajectories_seoul.RData')

i_p_s <- inf_results_s %>%
  mutate(specificity = factor(specificity, levels = c('0', '1', '2', '3', '4', '5')),
         num_points = factor(num_points, levels = c('one', 'two', 'three', 'four', 'five'))) %>%
  ggplot(aes(x = specificity, y = inf_prop, fill = factor(num_points))) +
  geom_boxplot(position = position_dodge(width = 0.5)) +
  labs(x = 'Number of decimals d',
       y = '',
       fill = '# EI Tuples\n',
       title = 'Seoul Only') +
  scale_fill_discrete(labels = c('One', 'Two', 'Three', 'Four', 'Five'),
                      guide = guide_legend(reverse = TRUE)) +
  theme(legend.position = c(0.025, 0.95),
        legend.justification = c('left', 'top'),
        legend.box.just = 'left',
        text = element_text(size = 12.5),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

addSmallLegend(i_p_s)
```

```{r fig.width=11, fig.height=7}
addSmallLegend(i_p) | addSmallLegend(i_p_s)
```

### Inference analysis for Counts.

Function to simulate an adversary having access to one location tuple corresponding to individual i and linking that tuple to the count for that tuple in an aggregated counts database to determine the COVID-19 status of that individual.
```{r}
inf_counts <- function(location_data, num_dec, prior = 0.01) {
  
  # dataframe for COVID-19 positive individuals
  pos_status_data <- tibble()
  
  # loop to guarantee we assign some COVID-19 positive individuals in the data
  # simulate 1% of individuals in Y as COVID-19 positive
  while(nrow(pos_status_data) == 0) {
  
    # add covid status to location data
    status_data <- location_data %>% 
      group_by(patient_id) %>%
      mutate(status = sample(c(0, 1), size = 1, prob = c(.99, .01))) %>%
      ungroup()
    
    # data corresponding to covid positive individuals (while loop condition just needs the number of rows > 0)
    pos_status_data <- status_data %>%
      filter(status == 1)

  }
  
  # round data to specified value of d
  rounded_status_data <- status_data %>%
    mutate_at(.vars = c('latitude', 'longitude'), round2, n = num_dec)
  
  # get all distinct location tuples for all COVID-19 positive individuals
  covid_locs <- rounded_status_data %>%
    filter(status == 1) %>%
    distinct(latitude, longitude)
  
  # get the regions from the rounded data that match the regions visited by the COVID-19 positive individuals
  # for each distinct region, calculate the LHS of the inference condition (the proportion of individuals with tuples in the region - prior
  count_props <- match_df(rounded_status_data, covid_locs, on = c('latitude', 'longitude')) %>%
    group_by(latitude, longitude) %>%
    summarize(inc_inf = mean(status) - prior, .groups = 'drop')

  # calculate the average of the LHS of the inference condition across the regions visited by each individual
  ind_avg_count_props <- pos_status_data %>%
    mutate_at(.vars = c('latitude', 'longitude'), round2, n = num_dec) %>% # round data corresponding to positive individuals
    distinct(patient_id, latitude, longitude) %>% # take the distinct regions visited by each positive individual
    left_join(count_props, by = c('latitude', 'longitude')) %>% # merge the LHS of inference condition calculated for each region in `count_props`
    group_by(patient_id) %>%
    summarize(avg_inc_inf = mean(inc_inf), .groups = 'drop') %>% # calculate the average across regions for each individual
    select(avg_inc_inf) %>%
    mutate(specificity = num_dec) # add variable indicating the number of decimals
    
  return(ind_avg_count_props)
    
}
```

Perform simulation 100 times for Full data for d = 0,1,2,3,4,5 and plot results.
```{r}
inf_count_dists <- map_dfr(0:5, function(y) map_dfr(1:100, function(x) inf_counts(full, y))) %>%
  mutate(specificity = factor(specificity, levels = c('0', '1', '2', '3', '4', '5')))

save(inf_count_dists, file = '../Data/counts_inference_full.RData')
load('../Data/counts_inference_full.RData')

count_inf_full <- inf_count_dists %>%
  ggplot(aes(x = specificity, y = avg_inc_inf)) +
  geom_boxplot() +
  labs(x = 'Number of decimals',
       y = 'Individual Level Average of LHS of Condition',
       title = 'Full Data') +
  scale_y_continuous(limits = c(-.05, 1), breaks = seq(0, 1, by = .25)) +
  theme(text = element_text(size = 12.5))
```

Repeat for Seoul.
```{r}
inf_count_dists_s <- map_dfr(0:5, function(y) map_dfr(1:100, function(x) inf_counts(s_full, y))) %>%
  mutate(specificity = factor(specificity, levels = c('0', '1', '2', '3', '4', '5')))

save(inf_count_dists_s, file = '../Data/counts_inference_seoul.RData')
load('../Data/counts_inference_seoul.RData')

count_inf_s <- inf_count_dists_s %>%
  ggplot(aes(x = specificity, y = avg_inc_inf)) +
  geom_boxplot() +
  labs(y = '',
       x = 'Number of decimals',
       title = 'Seoul Only') +
  scale_y_continuous(limits = c(-.05, 1), breaks = seq(0, 1, by = .25)) +
  theme(text = element_text(size = 12.5),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

```{r fig.width=11, fig.height=7}
count_inf_full | count_inf_s
```

```{r}
inf_count_dists_s %>%
  filter(specificity == 1) %>%
  summarize(m = max(avg_inc_inf))
```


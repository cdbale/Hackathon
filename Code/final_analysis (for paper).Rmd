---
title: "Untitled"
author: "Cameron Bale"
date: "8/29/2020"
output: html_document
---

Install and load packages.
```{r}
#install.packages(c('plyr', 'tidyverse', 'sp', 'patchwork', 'spatstat', 'ggmap', 'geosphere'))
library(plyr)
library(tidyverse)
library(sp)
library(patchwork)
library(spatstat)
library(ggmap)
library(geosphere)
library(sdcMicro)
library(FNN)
```

Load cleaned data.
```{r}
load(file = "../Data/korea_data_clean.RData")
options(digits = 10)
```

View latitude and longitude tuples.
```{r}
full %>% select(latitude, longitude)
```

How many individuals in the data?
```{r}
full %>%
  distinct(patient_id) %>%
  summarize(n = n())
```

What time frame does the data cover?
```{r}
full %>%
  summarize(min_date = min(date),
            max_date = max(date))
```

What are the minimum and maximum trajectory lengths?
```{r}
full %>%
  group_by(patient_id) %>%
  summarize(n = n(), .groups = 'drop') %>%
  summarize(min_length = min(n),
            max_length = max(n))
```

Distribution of trajectory lengths.
```{r}
full %>%
  group_by(patient_id) %>%
  summarize(n = n(), .groups = 'keep') %>%
  ggplot(aes(x = n)) +
  geom_histogram(binwidth = 1) +
  labs(title = 'Trajectory Length Distribution',
       y = 'Number of Individuals',
       x = "Number of Location Tuples in Trajectories") +
  theme(text = element_text(size = 12.5))
```

Create data for full korea data and seoul data only for individuals with at least five observed location points. We could have taken the observations with `province` == Seoul, but for the purposes of our analysis we wanted an area with higher location density. We take points in a rectangular region roughly centered on Seoul.
```{r}
full <- full %>%
  select(patient_id, latitude, longitude) %>%
  group_by(patient_id) %>%
  filter(n() > 4) %>%
  ungroup()

s_full <- full %>%
  filter((longitude > 126.85 & longitude < 127.15) & (latitude > 37.45 & latitude < 37.65)) %>%
  group_by(patient_id) %>%
  filter(n() > 4) %>%
  ungroup()
```

How many individuals after restriction.
```{r}
full %>%
  distinct(patient_id) %>%
  summarize(n = n())

s_full %>%
  distinct(patient_id) %>%
  summarize(n = n())
```

How many distinct location tuples, and what is the percentage of them are unique to one individual?
```{r}
(np <- full %>%
  distinct(latitude, longitude) %>%
  summarize(n = n()) %>%
  pull())

nup <- full %>%
  group_by(latitude, longitude) %>%
  summarize(n = n(), .groups = 'drop') %>%
  filter(n == 1) %>%
  summarize(n = n()) %>%
  pull()

nup/np * 100
```

What percentage of individuals have a unique location trajectory?
```{r}
split_trajs <- full %>%
  group_by(patient_id) %>%
  group_split(.keep = FALSE)

split_trajs <- lapply(split_trajs, function(x) arrange(x, latitude))

length(unique(split_trajs))/length(split_trajs) * 100
```

Calculate the density of points per square kilometer in the full dataset, excluding 16 points on an island South of the mainland (below 34 degrees latitude).
```{r}
# original data is in latitude and longitude
# have to project it onto a 2D space in terms of meters and divide by 1000 to get it in kilometers
# https://proj.org/operations/projections/utm.html
# it is a universal transverse mercator projection using the sp package
f <- full %>%
  select(longitude, latitude) %>%
  filter(latitude > 34)

coordinates(f) <- c("longitude", "latitude")
proj4string(f) <- CRS("+proj=longlat +datum=WGS84")
fm <- spTransform(f, CRS("+proj=utm +zone=52"))
fm <- as.data.frame(fm)/1000

# define window to calculate intensity in by the bounds created by the minimum and maximum observed latitude and longitude values
window <- data.frame(
  min_long = min(fm$longitude),
  min_lat = min(fm$latitude),
  max_long = max(fm$longitude),
  max_lat = max(fm$latitude)
)

# create ppp object to pass to intensity function
f_ppp <- ppp(fm$longitude, fm$latitude, c(window$min_long, window$max_long), c(window$min_lat, window$max_lat))

intensity(f_ppp)
```

Calculate the density of points per square kilometer in Seoul.
```{r}
s <- s_full %>%
  select(longitude, latitude)

coordinates(s) <- c("longitude", "latitude")
proj4string(s) <- CRS("+proj=longlat +datum=WGS84")
sm <- spTransform(s, CRS("+proj=utm +zone=52"))
sm <- as.data.frame(sm)/1000

s_window <- data.frame(
  min_long = min(sm$longitude),
  min_lat = min(sm$latitude),
  max_long = max(sm$longitude),
  max_lat = max(sm$latitude)
)

s_ppp <- ppp(sm$longitude, sm$latitude, c(s_window$min_long, s_window$max_long), c(s_window$min_lat, s_window$max_lat))

intensity(s_ppp)
```

```{r}
api_google <- "AIzaSyBTsLPmrNS8KUCi7ZDsCt_tIGFlL14Tbm0"

register_google(key=api_google)
```


Output maps of the full dataset and Seoul.
```{r fig.width = 10, fig.height = 8}
k_map <- ggmap(get_map(location = c(125, 33, 131, 38.5)), 
               extent = 'panel', 
               base_layer = ggplot(full, aes(x = longitude, y = latitude))) +
  geom_point(alpha = 0.4, color = 'blue', size=2) +
  labs(x = 'Longitude',
       y = 'Latitude')

print(k_map)
```

```{r}
s_map <- ggmap(get_map(location = c(126.85, 37.45, 127.15, 37.65)), 
               extent = 'panel', 
               base_layer = ggplot(s_full, aes(x = longitude, y = latitude))) +
  geom_point(alpha = 0.4, color = 'blue', size=2) +
  labs(x = '',
       y = '')

print(s_map)
```

Define a custom rounding function. Numbers ending in 1 - 4 get rounded down, numbers ending in 5 - 9 get rounded up.
```{r}
round2 = function(x, n) {
  rx = abs(x)*10^n
  rx = rx + 0.5 + sqrt(.Machine$double.eps)
  rx = trunc(rx)
  rx = rx/10^n
  return(rx)
}
```

Create maps showing coarsened trajectories.
```{r fig.width = 10, fig.height = 8}
traj_1d <- full %>%
  mutate(latitude = round2(latitude, n = 1),
         longitude = round2(longitude, n = 1))

coarse_k_map <- ggmap(get_map(location = c(125, 33, 131, 38.5)), 
               extent = 'panel', 
               base_layer = ggplot(traj_1d, aes(x = longitude, y = latitude))) +
  geom_point(alpha = 0.4, color = 'blue', size=2) +
  labs(x = 'Longitude',
       y = 'Latitude')

print(coarse_k_map)
```

```{r}
traj_1ds <- s_full %>%
  mutate(latitude = round2(latitude, n = 1),
         longitude = round2(longitude, n = 1))

coarse_s_map <- ggmap(get_map(location = c(126.85, 37.45, 127.15, 37.65)), 
               extent = 'panel', 
               base_layer = ggplot(traj_1ds, aes(x = longitude, y = latitude))) +
  geom_point(color = 'blue', size=3) +
  labs(x = '',
       y = '')

print(coarse_s_map)
```


```{r}
traj_1ds <- s_full %>%
  mutate(latitude = round2(latitude, n = 2),
         longitude = round2(longitude, n = 2))

coarse_s_map <- ggmap(get_map(location = c(126.85, 37.45, 127.15, 37.65)), 
               extent = 'panel', 
               base_layer = ggplot(traj_1ds, aes(x = longitude, y = latitude))) +
  geom_point(color = 'blue', size=3) +
  labs(x = '',
       y = '')

print(coarse_s_map)
```


```{r}
traj_1ds <- s_full %>%
  mutate(latitude = round2(latitude, n = 0),
         longitude = round2(longitude, n = 0))

coarse_s_map <- ggmap(get_map(location = c(126, 37, 128, 38)), 
               extent = 'panel', 
               base_layer = ggplot(traj_1ds, aes(x = longitude, y = latitude))) +
  geom_point(color = 'blue', size=3) +
  labs(x = '',
       y = '')

print(coarse_s_map)
```

Calculating distance between points for individual i (From paper).
```{r}
distGeo(c(127.0170, 37.59256), c(127.017, 37.593))
```

Perform microaggregation on latitude and longitude variables for k = 5.
```{r}
agg_full <- microaggregation(full, variables=c("latitude", "longitude"), aggr=5)

agg_full$mx 
```


```{r fig.width = 10, fig.height = 8}
agg_map <- ggmap(get_map(location = c(125, 33, 131, 38.5)), 
               extent = 'panel', 
               base_layer = ggplot(agg_full$mx, aes(x = longitude, y = latitude))) +
  geom_point(alpha = 0.4, color = 'blue', size=2) +
  labs(x = 'Longitude',
       y = 'Latitude')

print(agg_map)
```



```{r}
agg_s <- microaggregation(s_full, variables=c("latitude", "longitude"), aggr=5)

agg_s_map <- ggmap(get_map(location = c(126.85, 37.45, 127.15, 37.65)), 
               extent = 'panel', 
               base_layer = ggplot(agg_s$mx, aes(x = longitude, y = latitude))) +
  geom_point(color = 'blue', size=2) +
  labs(x = '',
       y = '')

print(agg_s_map)
```



### Singling-out: Location Coarsening

Define `trajectory_uniqueness` function. Determines the percentage of unique trajectories in a simulated database. The simulated database is created by randomly sampling `num_points` points from each trajectory in `trajectory_data`.
```{r}
trajectory_uniqueness <- function(trajectory_data, num_points) {
  
  # calculate the total number of trajectories in the data
  num_trajectories <- length(unique(trajectory_data$patient_id))
  
  # randomly sample 'num_points' points from each trajectory
  # split into list - each element is a sampled trajectory (patient_id is dropped)
  sampled_data <- trajectory_data %>%
    group_by(patient_id) %>%
    slice_sample(n = num_points) %>%
    group_split(.keep = FALSE)
  
  # order each trajectory based on 'latitude' values - necessary for trajectory comparison using 'unique' function
  sampled_data <- lapply(sampled_data, function(x) arrange(x, latitude))
  
  # calculate the percentage of sampled trajectories that are unique
  percent_unique <- 100 * (length(unique(sampled_data))/num_trajectories)
  
  return(as_tibble(percent_unique))
  
}
```

Define `uniqueness_sims` function. Use this function to perform multiple simulations for different numbers of sampled trajectory points (`sample_size`) for a given number of decimals at which locations are measured (`num_dec`).
```{r}
uniqueness_sims <- function(location_data, num_dec, nits = 100, sample_size = 1:5, pvar_names = c('one', 'two', 'three', 'four', 'five')) {
  
  # round latitude and longitude to the specified number of decimals
  loc_data <- location_data %>%
    mutate_at(.vars = c('latitude', 'longitude'), round2, n = num_dec)
  
  # calculate the percentage of unique trajectories in 'nits' simulated databases for each 'sample_size'
  unique_pcts <- map_dfc(sample_size, function(y) map_dfr(1:nits, function(x) trajectory_uniqueness(loc_data, y)))
  
  # assign column names to mark the sample size
  colnames(unique_pcts) <- pvar_names
  
  # transform into long format - one column for the number of sampled points, one for the percentage of unique trajectories, 
  # and add a column for the number of decimals
  # each row is for one simulated database, i.e. unique_res[0,] = (number_of_sampled_points, percent_unique_trajectories, number of decimals)
  unique_res <- unique_pcts %>%
    gather(key = 'num_points', value = 'percent_unique') %>%
    mutate(specificity = num_dec)
  
  return(unique_res)
  
}
```

Perform simulation 100 times for sample sizes of one to five points, for locations measured from five to zero decimals.
```{r}
# uniqueness_results_full <- uniqueness_sims(full, 5) %>%
#   bind_rows(uniqueness_sims(full, 4)) %>%
#   bind_rows(uniqueness_sims(full, 3)) %>%
#   bind_rows(uniqueness_sims(full, 2)) %>%
#   bind_rows(uniqueness_sims(full, 1)) %>%
#   bind_rows(uniqueness_sims(full, 0))
```

The simulation takes a long time to run, so save the results.
```{r}
# save(uniqueness_results_full, file = '../Data/uniqueness_results_full.RData')
```

Load uniqueness calculations for full data (can do this after running the above chunks once).
```{r}
load('../Data/uniqueness_results_full.RData')
```

Repeat for Seoul.
```{r}
# uniqueness_results_seoul <- uniqueness_sims(s_full, 5) %>%
#   bind_rows(uniqueness_sims(s_full, 4)) %>%
#   bind_rows(uniqueness_sims(s_full, 3)) %>%
#   bind_rows(uniqueness_sims(s_full, 2)) %>%
#   bind_rows(uniqueness_sims(s_full, 1)) %>%
#   bind_rows(uniqueness_sims(s_full, 0))
```

The simulation takes a long time to run, so save the results.
```{r}
# save(uniqueness_results_seoul, file = '../Data/uniqueness_results_seoul.RData')
```

Load uniqueness calculations for seoul data (can do this after running the above chunk once).
```{r}
load('../Data/uniqueness_results_seoul.Rdata')
```

Plot uniqueness results for South Korea and Seoul side by side.
```{r fig.width = 10, fig.height = 8}
boxes_full <- uniqueness_results_full %>%
  mutate(specificity = factor(specificity, levels = c('0', '1', '2', '3', '4', '5')),
         num_points = factor(num_points, levels = c('one', 'two', 'three', 'four', 'five'))) %>%
  ggplot(aes(x = specificity, y = percent_unique, fill = factor(num_points))) +
  geom_boxplot(position = position_dodge(width = 0)) +
  labs(x = 'Number of Decimals',
       y = '% Unique Trajectories',
       fill = '# Tuples\n',
       title = 'Full Data') +
  scale_fill_discrete(labels = c('One', 'Two', 'Three', 'Four', 'Five'),
                      guide = guide_legend(reverse = TRUE)) +
  theme(legend.position = c(.9, .1),
        legend.justification = c('right', 'bottom'),
        legend.box.just = 'right',
        text = element_text(size = 12.5)) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, by = 25))

boxes_s <- uniqueness_results_seoul %>%
  mutate(specificity = factor(specificity, levels = c('0', '1', '2', '3', '4', '5')),
         num_points = factor(num_points, levels = c('one', 'two', 'three', 'four', 'five'))) %>%
  ggplot(aes(x = specificity, y = percent_unique, fill = factor(num_points))) +
  geom_boxplot(position = position_dodge(width = 0)) +
  labs(x = 'Number of Decimals',
       y = '',
       fill = '# Tuples\n',
       title = 'Seoul Only') +
  scale_fill_discrete(labels = c('One', 'Two', 'Three', 'Four', 'Five'),
                      guide = guide_legend(reverse = TRUE)) +
  theme(legend.position = c(.9, .1),
        legend.justification = c('right', 'bottom'),
        legend.box.just = 'right',
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        text = element_text(size = 12.5)) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, by = 25))

addSmallLegend <- function(myPlot, pointSize = 2, textSize = 10, spaceLegend = 1) {
    myPlot +
        guides(shape = guide_legend(override.aes = list(size = pointSize)),
               color = guide_legend(override.aes = list(size = pointSize))) +
        theme(legend.title = element_text(size = textSize), 
              legend.text  = element_text(size = textSize),
              legend.key.size = unit(spaceLegend, "lines"))
}

addSmallLegend(boxes_full) | addSmallLegend(boxes_s)
```

Calculate the average percentage of unique trajectories across all simulations at each value of d for the full data.
```{r}
uniqueness_results_full %>% group_by(specificity) %>% summarize(avg_pct_unique = mean(percent_unique), .groups = 'drop')
```

Calculate the average percentage of unique trajectories across all simulations at each value of d for the Seoul data.
```{r}
uniqueness_results_seoul %>% group_by(specificity) %>% summarize(avg_pct_unique = mean(percent_unique), .groups = 'drop')
```

```{r}
ma_k <- c(2, 5, 10, 15, 20, 25)
```


Singling out analysis for microaggregation.
```{r}
ma_data_sets <- lapply(ma_k, function(x) microaggregation(full, variables=c("latitude", "longitude"), aggr=x)$mx)

mas_data_sets <- lapply(ma_k, function(x) microaggregation(s_full, variables=c("latitude", "longitude"), aggr=x)$mx)
```

UTILITY EVALUTAION

Function to calculate the distances shifted by microaggregation.
```{r}
micro_distances_shifted <- function(X, mX){
  ds <- X %>%
    bind_cols(mX) %>%
    mutate(distances = distGeo(tibble(`longitude...3`, `latitude...2`), tibble(`longitude...5`, `latitude...4`))) %>%
    pull(distances)
  
  return(ds)
}
```

Function to calculate the distribution of distances that points are shifted by location coarsening.
```{r}
distances_shifted <- function(location_data, num_dec) {
  
  ds <- location_data %>%
    mutate(latitude2 = round2(latitude, n = num_dec),
           longitude2 = round2(longitude, n = num_dec),
           distances = distGeo(tibble(longitude, latitude), tibble(longitude2, latitude2))) %>%
    select(distances)
  
  return(ds)
  
}
```

Calculate distances shifted for microaggregated data.
```{r}
ma_distances <- map(ma_data_sets, function(x) micro_distances_shifted(full, x))

mas_distances <- map(mas_data_sets, function(x) micro_distances_shifted(s_full, x))
```

Calculate distances shifted for location tuples measured from zero to four decimals.
```{r}
full_distances <- map_dfc(0:5, function(x) distances_shifted(full, x))
seoul_distances <- map_dfc(0:5, function(x) distances_shifted(s_full, x))
```

We also make a function to display the quantiles of the distances that observed location points were shifted when we round `latitude` and `longitude`.
```{r}
dist_quants <- function(distances){
  
  qs <- tibble(
    Min = min(distances),
    `2.5%` = quantile(distances, probs = .025),
    `50%` = median(distances),
    `97.5%` = quantile(distances, probs = 0.975),
    Max = max(distances)
  )
  
  return(qs)
  
}
```

Calculate the quantiles of the distributions of distances shifted by microaggregation for the full data.
```{r}
map_dfr(ma_distances, dist_quants)  %>%
  round(digits=2) %>%
  select(`2.5%`, `50%`, `97.5%`)
```

Calculate the quantiles of the distributions of distances shifted by microaggregation for the Seoul data.
```{r}
map_dfr(mas_distances, dist_quants) %>%
  round(digits=2) %>%
  select(`2.5%`, `50%`, `97.5%`)
```


Calculate the quantiles of the distributions of distances shifted for locations measured from zero to four decimals.
```{r}
map_dfr(full_distances, function(x) dist_quants(x)) %>%
  mutate(Specificity = 0:5) %>%
  mutate_at(.vars = c('Min', '2.5%', '50%', '97.5%', 'Max'), round2, n = 2)
```

```{r}
map_dfr(seoul_distances, function(x) dist_quants(x)) %>%
  mutate(Specificity = 0:5) %>%
  mutate_at(.vars = c('Min', '2.5%', '50%', '97.5%', 'Max'), round2, n = 2)
```

<!-- ### Singling-out: Aggregated Counts -->

<!-- No simulation here - just the percent and number of regions containing at least two location tuples. -->
<!-- ```{r} -->
<!-- bin_dist <- function(loc_data, num_dec) { -->

<!--   # obtain the number of times each location tuple occurs - each tuple marks the center of a region - number of occurrences of a tuple is the number of points originally in that region. Select the variable with the number of occurences for each tuple, and create a variable 'd' to indicate the number of decimals -->
<!--   loc_data %>% -->
<!--     mutate(latitude = round2(latitude, num_dec), -->
<!--            longitude = round2(longitude, num_dec)) %>% -->
<!--     group_by(latitude, longitude) %>%  -->
<!--     summarize(n = n(), .groups = 'drop') %>% -->
<!--     select(n) %>%                            -->
<!--     mutate(d = factor(num_dec)) -->

<!-- } -->

<!-- # perform calculation on full data for values of d from 5 to 0 -->
<!-- bin_dists <- bin_dist(full, num_dec = 5) %>% -->
<!--   bind_rows(bin_dist(full, num_dec = 4), -->
<!--             bin_dist(full, num_dec = 3), -->
<!--             bin_dist(full, num_dec = 2), -->
<!--             bin_dist(full, num_dec = 1), -->
<!--             bin_dist(full, num_dec = 0)) -->

<!-- # create variable indicating regions with more than one location tuple -->
<!-- # calculate the percentage and number of regions with more than one location tuple -->
<!-- bin_dists %>% -->
<!--   mutate(not_singled_out = if_else(n > 1, 1, 0)) %>% -->
<!--   group_by(d) %>% -->
<!--   summarize(pct_nso = mean(not_singled_out) * 100, -->
<!--             num_nso = sum(not_singled_out), .groups = 'drop') -->
<!-- ``` -->

<!-- Repeat for Seoul. -->
<!-- ```{r} -->
<!-- bin_dists_s <- bin_dist(s_full, num_dec = 5) %>% -->
<!--   bind_rows(bin_dist(s_full, num_dec = 4), -->
<!--             bin_dist(s_full, num_dec = 3), -->
<!--             bin_dist(s_full, num_dec = 2), -->
<!--             bin_dist(s_full, num_dec = 1), -->
<!--             bin_dist(s_full, num_dec = 0)) -->

<!-- bin_dists_s %>% -->
<!--   mutate(not_singled_out = if_else(n > 1, 1, 0)) %>% -->
<!--   group_by(d) %>% -->
<!--   summarize(pct_nso = mean(not_singled_out) * 100, -->
<!--             num_nso = sum(not_singled_out), .groups = 'drop') -->
<!-- ``` -->

<!-- Calculate the areas and side lengths of all regions in the data. -->
<!-- ```{r} -->
<!-- # make a function for calculating polygon boundaries and area -->
<!-- # designed to take vectors or dataframe columns of latitude and longitude values. Reports the area over which points would -->
<!-- # have been coarsened to the corresponding lat/long point -->
<!-- # this function is specific to this data i.e. it is assumed that latitude originally had 5 decimals, and longitude had 4 -->
<!-- poly_area <- function(loc_data, num_dec) { -->

<!--   # create point data -->
<!--   pt <- loc_data %>% select(-patient_id) %>% mutate_all(round2, n = num_dec) -->

<!--   # get the minimum latitude/longitude values that correspond to the edges of the polygon -->
<!--   pt$min_lat <- pt$latitude - as.numeric(paste0(".", str_pad("5", num_dec + 1, pad = "0"))) -->
<!--   pt$min_long <- pt$longitude - as.numeric(paste0(".", str_pad("5", num_dec + 1, pad = "0"))) -->

<!--   # get the 'maximum' latitude/longitude values that correspond to the edges of the polygon (going out to 7 decimal places) -->
<!--   pt$max_lat <- pt$latitude + as.numeric(paste0(".", str_pad(str_pad("4", num_dec + 1, pad = "0"), 7, pad = "9", side = "right"))) -->
<!--   pt$max_long <- pt$longitude + as.numeric(paste0(".", str_pad(str_pad("4", num_dec + 1, pad = "0"), 7, pad = "9", side = "right"))) -->

<!--   A <- data.frame(apply(pt[,3:6], 1, function(x)  -->
<!--   {pol <- rbind(c(x[2], x[1]), -->
<!--                 c(x[2], x[3]), -->
<!--                 c(x[4], x[3]), -->
<!--                 c(x[4], x[1]), -->
<!--                 c(x[2], x[1])) -->
<!--   areaPolygon(pol)})) -->

<!--   names(A) <- "area" -->

<!--   # vertical (or latitudinal) lengths are equal, only need to take one -->
<!--   A$vert_length <- distGeo(data.frame(pt$max_long, pt$max_lat), data.frame(pt$max_long, pt$min_lat)) -->

<!--   # horizontal (or longitudinal) lengths are slightly different, the northern side will be slightly shorter -->
<!--   # we take the average of the two -->
<!--   A$hor_length <- apply(data.frame(distGeo(data.frame(pt$max_long, pt$max_lat), data.frame(pt$min_long, pt$max_lat)), -->
<!--                                    distGeo(data.frame(pt$max_long, pt$min_lat), data.frame(pt$min_long, pt$min_lat))), -->
<!--                         1, mean) -->

<!--   return(A) -->

<!-- } -->
<!-- ``` -->

<!-- Function to calculate average side lengths and area for regions. -->
<!-- ```{r} -->
<!-- poly_stats <- function(trajectory_data) { -->
<!--   trajectory_data %>% -->
<!--   summarize(hor_length = mean(hor_length), -->
<!--             vert_length = mean(vert_length), -->
<!--             p_area_m = mean(area), -->
<!--             p_area_km = mean(area)/1000000) -->
<!-- } -->
<!-- ``` -->

<!-- Calculate areas and side lengths for regions for values of d = 0 to 5. -->
<!-- ```{r} -->
<!-- map_dfr(0:5, function(x) poly_stats(poly_area(full, num_dec = x))) %>% -->
<!--   mutate(number_decimals = 0:5) %>%  -->
<!--   mutate_at(.vars = c('hor_length', 'vert_length', 'p_area_km'), round2, n = 2) %>% -->
<!--   mutate_at(.vars = 'p_area_m', round2, n = 1) -->
<!-- ``` -->

### Linkability Risk measure for Location Coarsening

```{r}
linkability_risk <- function(trajectory_data, num_points) {
  
  # calculate the total number of trajectories in the data
  num_trajectories <- length(unique(trajectory_data$patient_id))
  
  # randomly sample 'num_points' points from each trajectory
  # split into list - each element is a sampled trajectory (patient_id is dropped)
  sampled_data <- trajectory_data %>%
    group_by(patient_id) %>%
    slice_sample(n = num_points) %>%
    group_split(.keep = FALSE)
  
  # order each trajectory based on 'latitude' values - necessary for trajectory comparison using 'unique' function
  sampled_data <- lapply(sampled_data, function(x) arrange(x, latitude))
  
  num_occurrences <- tabulate(match(sampled_data, unique(sampled_data)))
  
  # num_occurrences <- f3(sampled_data)
  
  # # logical vector of duplicated trajectories - these are the ones for which k >= 2
  # is_duplicate <- duplicated(sampled_data)
  # 
  # # select duplicated trajectories
  # duplicates <- sampled_data[is_duplicate]
  # 
  # # reduce to subset of unique duplicated trajectories
  # unique_duplicates <- unique(duplicates)
  # 
  # # for each unique duplicated trajectory, count how many times it occurs in the full sampled data
  # num_occurrences <- c()
  # if (length(unique_duplicates) > 0){
  #   for (i in 1:length(unique_duplicates)){
  #     num_matches <- 0
  #     for (j in 1:length(sampled_data)){
  #       if (all(unique_duplicates[[i]] == sampled_data[[j]])){
  #         num_matches <- num_matches + 1
  #       }
  #     }
  #     num_occurrences <- c(num_occurrences, num_matches)
  #   }
  # }
  
  return(num_occurrences[num_occurrences>1])
  
}
```

Linkability risk simulations.
```{r}
linkability_sims <- function(location_data, num_dec, nits = 100, sample_size = 1:5, pvar_names = c('one', 'two', 'three', 'four', 'five')) {
  
  # round latitude and longitude to the specified number of decimals
  loc_data <- location_data %>%
    mutate_at(.vars = c('latitude', 'longitude'), round2, n = num_dec)
  
  # calculate the percentage of unique trajectories in 'nits' simulated databases for each 'sample_size'
  unique_pcts <- map(sample_size, function(y) map(1:nits, function(x) linkability_risk(loc_data, y)))
  
  vecs <- tibble()
  for (i in seq_along(sample_size)){
    lvec <- c()
    for (j in 1:nits){
      lvec <- c(lvec, unique_pcts[[i]][[j]])
    }
    specificity <- rep(num_dec, times=length(lvec))
    num_points <- rep(i, times=length(lvec))
    new_df <- bind_cols("num_points"=num_points, "k"=lvec, "specificity"=specificity)
    vecs <- bind_rows(vecs, new_df)
  }
  
  return(vecs)
  
}
```

```{r}
# t1 <- Sys.time()
# nits=100
# linkability_results_full <- linkability_sims(full, 5, nits=nits) %>%
#   bind_rows(linkability_sims(full, 4, nits=nits)) %>%
#   bind_rows(linkability_sims(full, 3, nits=nits)) %>%
#   bind_rows(linkability_sims(full, 2, nits=nits)) %>%
#   bind_rows(linkability_sims(full, 1, nits=nits)) %>%
#   bind_rows(linkability_sims(full, 0, nits=nits))
# t2 <- Sys.time()
# t1-t2

# save(linkability_results_full, file = '../Data/linkability_results_full.RData')

load("../Data/linkability_results_full.RData")
```

```{r}
# linkability_results_seoul <- linkability_sims(s_full, 5, nits=nits) %>%
#   bind_rows(linkability_sims(s_full, 4, nits=nits)) %>%
#   bind_rows(linkability_sims(s_full, 3, nits=nits)) %>%
#   bind_rows(linkability_sims(s_full, 2, nits=nits)) %>%
#   bind_rows(linkability_sims(s_full, 1, nits=nits)) %>%
#   bind_rows(linkability_sims(s_full, 0, nits=nits))
# 
# save(linkability_results_seoul, file = '../Data/linkability_results_seoul.RData')

load("../Data/linkability_results_seoul.RData")
```

Plot of linkability simulation results for full data.
```{r fig.width = 10, fig.height = 8}
linkability_plot_full <- linkability_results_full %>%
  mutate(specificity = factor(specificity, levels = c('0', '1', '2', '3', '4', '5')),
         num_points = factor(num_points, levels = c(1, 2, 3, 4, 5), labels=c("one", "two", "three", "four", "five"))) %>%
  ggplot(aes(x = specificity, y = 1/k, fill = factor(num_points))) +
  geom_boxplot(position = position_dodge(width = 0.5)) +
  labs(x = 'Number of Decimals',
       y = '1/k',
       fill = '# Tuples\n',
       title = 'Full Data') +
  scale_fill_discrete(labels = c('One', 'Two', 'Three', 'Four', 'Five'),
                      guide = guide_legend(reverse = TRUE)) +
  theme(legend.position = "none",
        text = element_text(size = 12.5)) +
  scale_y_continuous(limits = c(0, 0.5), breaks = seq(0, 0.5, by = 0.1))

linkability_plot_seoul <- linkability_results_seoul %>%
  mutate(specificity = factor(specificity, levels = c('0', '1', '2', '3', '4', '5')),
         num_points = factor(num_points, levels = c(1, 2, 3, 4, 5), labels=c("one", "two", "three", "four", "five"))) %>%
  ggplot(aes(x = specificity, y = 1/k, fill = factor(num_points))) +
  geom_boxplot(position = position_dodge(width = 0.5)) +
  labs(x = 'Number of Decimals',
       y = '',
       fill = '# Tuples\n',
       title = 'Seoul Only') +
  scale_fill_discrete(labels = c('One', 'Two', 'Three', 'Four', 'Five'),
                      guide = guide_legend(reverse = TRUE)) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        text = element_text(size = 12.5)) +
  scale_y_continuous(limits = c(0, 0.5), breaks = seq(0, 0.5, by = 0.1))


addSmallLegend(linkability_plot_full) | addSmallLegend(linkability_plot_seoul)
```


### Linkability Risk for Microaggregation

Compute 1/k for all points
```{r}
ma_linkability_risk_calc <- function(X, ds_num, cluster_size){
  temp <- X %>%
    group_by(latitude, longitude) %>%
    summarize(risk = 1/n(), .groups='drop') %>%
    mutate(k=cluster_size)
  
  return(temp)
}

ma_full_link <- map_dfr(1:length(ma_data_sets), function(x) ma_linkability_risk_calc(ma_data_sets[[x]], x, ma_k[x]))

ma_full_link <- ma_full_link %>%
  mutate(k <- factor(k, levels=rev(ma_k)))

ma_s_link <- map_dfr(1:length(mas_data_sets), function(x) ma_linkability_risk_calc(mas_data_sets[[x]], x, ma_k[x]))

ma_s_link <- ma_s_link %>%
  mutate(k <- factor(k, levels=rev(ma_k)))
```

Plot
```{r fig.width = 10, fig.height = 8}
full_ma_link_plot <- ma_full_link %>%
  ggplot(aes(x=as.factor(k), y=risk)) +
  geom_boxplot() +
  labs(x = 'k',
       y = '1/k',
       title = 'Full Data') +
  scale_x_discrete(limits=rev) +
  scale_y_continuous(limits = c(0, 0.5), breaks = seq(0, 0.5, by = 0.1))
  

s_ma_link_plot <- ma_s_link %>%
  ggplot(aes(x=as.factor(k), y=risk)) +
  geom_boxplot() +
  labs(x = 'k',
       y = '',
       title = 'Seoul Only') +
  scale_x_discrete(limits=rev) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  scale_y_continuous(limits = c(0, 0.5), breaks = seq(0, 0.5, by = 0.1))

(addSmallLegend(linkability_plot_full) + addSmallLegend(linkability_plot_seoul)) / (full_ma_link_plot + s_ma_link_plot)
```









<!-- ### Linkability Risk measure for Aggregating to Counts -->

<!-- ```{r fig.width = 10, fig.height = 8} -->
<!-- lr_agg_full <- bin_dists %>% -->
<!--   mutate(d = factor(d, levels = c(0,1,2,3,4,5)), -->
<!--          k = n) %>% -->
<!--   filter(k > 1) %>% -->
<!--   ggplot(aes(x=d, y=1/k)) + -->
<!--   geom_boxplot() + -->
<!--   labs(x = "Number of Decimals", -->
<!--        title = "Full Data") + -->
<!--   theme(text = element_text(size = 12.5)) -->

<!-- lr_agg_seoul <- bin_dists_s %>% -->
<!--   mutate(d = factor(d, levels = c(0,1,2,3,4,5)), -->
<!--          k = n) %>% -->
<!--   filter(k > 1) %>% -->
<!--   ggplot(aes(x=d, y=1/k)) + -->
<!--   geom_boxplot() + -->
<!--   labs(x = "Number of Decimals", -->
<!--        y = "", -->
<!--        title = "Seoul Only") + -->
<!--   theme(axis.text.y = element_blank(), -->
<!--         axis.ticks.y = element_blank(), -->
<!--         text = element_text(size = 12.5)) -->

<!-- addSmallLegend(lr_agg_full) | addSmallLegend(lr_agg_seoul) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- bin_dists_s %>% -->
<!--   filter(n > 1) %>% -->
<!--   group_by(d) %>% -->
<!--   select(n) %>% -->
<!--   summarize(inverse_n = 1/n) %>% -->
<!--   table() -->

<!-- seoul_agg_bins <- bin_dists_s %>% -->
<!--   filter(n > 1) %>% -->
<!--   select(n) %>% -->
<!--   summarize(inverse_n = 1/n) %>% -->
<!--   table() -->

<!-- # amount in top bins -->
<!-- seoul_agg_bins[(length(seoul_agg_bins)-9):length(seoul_agg_bins)] -->

<!-- # remainder -->
<!-- sum(seoul_agg_bins[1:(length(seoul_agg_bins)-10)]) -->
<!-- ``` -->


### Assessing Inference for Trajectories

Function to simulate an adversary having external information location tuples corresponding to COVID-19 positive individuals in a database. The adversary links these external information tuples to all compatible trajectories in Y. The disease status would then be inferred based on the proportion of individuals with compatible trajectories that are COVID-19 positive. This proportion is used to to calculate the left-hand side (LHS) of the inference condition from the paper.


###############################################################################
###############################################################################
Not sure if we want to prevent all risks or just look at each individually.
###############################################################################
###############################################################################

Perform 500 simulations for each value of d:
  Perform 100 simulations for each trajectory length:
  (1) simulate positive COVID-19 status for 1% of individuals
  (2) sample traj_length points (without replacement) from each trajectory 
  (3) sample one point from each positive trajectory and treat as EI
  (4) remove unique records (locations + status)
  (5) link EI points to trajectories (think about what to do if there is no match)
  (6) calculate inference

We assume if the adversary does not find a matching tuple, they do not perform
inference, i.e., the increase in inference is 0.

```{r}
loc_data <- full %>%
  mutate_at(.vars=c("latitude", "longitude"), round2, n=1)

traj_length <- 3

prior = 0.01
```


```{r}
## (1) 
  
# number of COVID-19 positive individuals in the database
pos_status_data <- tibble()
  
# loop to guarantee we assign some COVID-19 positive individuals in the data
# simulate 1% of individuals in Y as COVID-19 positive
while(nrow(pos_status_data) == 0) {
  
  # add covid status to location data
  status_data <- loc_data %>% 
    group_by(patient_id) %>%
    mutate(status = sample(c(0, 1), size = 1, prob = c(.99, .01))) %>%
    ungroup()
    
  # data corresponding to covid positive individuals (while loop condition just needs the number of rows > 0)
  pos_status_data <- status_data %>%
    filter(status == 1)

}

## (2)
  
# randomly sample 'traj_length' points from each trajectory
# split into list - each element is a sampled trajectory (patient_id is dropped)
sampled_data <- status_data %>%
  group_by(patient_id) %>%
  slice_sample(n = traj_length) %>%
  ungroup()
  

## (3)
ei <- sampled_data %>%
  filter(status == 1) %>%
  select(-status) %>%
  group_by(patient_id) %>%
  slice_sample(n=1) %>%
  group_split(.keep=FALSE)

## (4)

# take 'original' data, split on a per-trajectory basis, and remove
# unique trajectory/status combinations

sampled_data <- sampled_data %>%
  group_by(patient_id) %>%
  group_split()



is_matched <- lapply(ei, function(y) sapply(sampled_data, function(x) nrow(match_df(y, x, on=c("latitude", "longitude"))) > 0))

max_increase <- max(sapply(is_matched, function(x) mean(sapply(sampled_data[x], function(y) unique(y$status)))))
```





```{r}
# function takes coarsened data as input
inference_simulation <- function(loc_data, traj_length, prior = 0.01) {
  
  # ## (1) 
  # 
  # # number of COVID-19 positive individuals in the database
  # pos_status_data <- tibble()
  # 
  # # loop to guarantee we assign some COVID-19 positive individuals in the data
  # # simulate 1% of individuals in Y as COVID-19 positive
  # while(nrow(pos_status_data) == 0) {
  # 
  #   # add covid status to location data
  #   status_data <- loc_data %>% 
  #     group_by(patient_id) %>%
  #     mutate(status = sample(c(0, 1), size = 1, prob = c(.99, .01))) %>%
  #     ungroup()
  #   
  #   # data corresponding to covid positive individuals (while loop condition just needs the number of rows > 0)
  #   pos_status_data <- status_data %>%
  #     filter(status == 1)
  # 
  # }
  # 
  # ## (2)
  # 
  # # randomly sample 'traj_length' points from each trajectory
  # # split into list - each element is a sampled trajectory (patient_id is dropped)
  # sampled_data <- status_data %>%
  #   group_by(patient_id) %>%
  #   slice_sample(n = traj_length) %>%
  #   ungroup()
  # 
  # 
  # ## (3)
  # ei <- sampled_data %>%
  #   filter(status == 1) %>%
  #   group_by(patient_id) %>%
  #   slice_sample(n=1) %>%
  #   ungroup()
  # 
  # ## (4)
  # 
  # # take 'original' data, split on a per-trajectory basis, and remove
  # # unique trajectory/status combinations
  # 
  # sampled_data <- sampled_data[duplicated(sampled_data)]
  # 
  # 
  # 
  # 
  # 
  # 
  # 
  # 
  # 
  # 
  # 
  # 
  # # sample 'ci_length' tuples from the trajectory of each COVID-19 positive individual
  # # these tuples will represent the external information available to an adversary
  # # the sample from each trajectory is a dataframe stored as a list element in 'ext_inf'
  # ext_inf <- sampled_data %>%
  #   group_by(patient_id) %>%
  #   sample_n(size = ci_length) %>%
  #   group_split()
  # 
  # # remove patient id and status from external information samples
  # ext_inf2 <- lapply(ext_inf, function(x) x %>% select(-patient_id, -status))
  # 
  # # summarize each external information dataframe so that it contains one row for each unique external information tuple
  # # in a given sample - add a variable for how many times each unique tuple occurred in the original sample
  # ext_inf3 <- lapply(ext_inf2, function(x) x %>% group_by(latitude, longitude) %>% summarize(n = n(), .groups = 'drop'))
  # 
  # # extract the tuples from the original full database that match the tuples in the external information samples - perform a similar summary on the matches - create one row for each unique tuple on the basis of `patient_id` with the number of times each tuple occurs in that patient's trajectory
  # matches <- lapply(ext_inf3, function(y) match_df(status_data, y, on = c('latitude', 'longitude')) %>%
  #                                         group_by(patient_id, latitude, longitude, status) %>%
  #                                         summarize(n1 = n(), .groups = 'drop'))
  # 
  # # merge the variables for patient_id, disease_status, and the number of times a given tuple occurs in the patient trajectory with the external information samples - we can now compare the number of times tuples occur in the external information sample to the number of times they occur in a patient's trajectory
  # links <- lapply(1:length(ext_inf3), function(x) left_join(matches[[x]], ext_inf3[[x]], by = c('latitude', 'longitude')))
  # 
  # # filter for the patients with trajectories that contain the same unique tuples occurring at least as many times as the original external information sample
  # links2 <- lapply(1:length(links), function(x) links[[x]] %>% group_by(patient_id) %>% filter(n1 >= n & n() >= nrow(ext_inf3[[x]])) %>% ungroup())
  # 
  # # reduce to list of dataframes with patient_ids and COVID-19 status corresponding to the trajectories that were linked to each external information sample
  # covid_count <- lapply(links2, function(x) distinct(x, patient_id, status))
  # 
  # # calculate left hand side of the inference condition for each external information sample
  # inf_props <- map_dfr(covid_count, function(x) x %>% summarize(cond = mean(status) - prior))
  # 
  # # average the left hand side of the inference condition across all COVID-19 positive individuals
  # max_cond <- inf_props %>% summarize(max_cond = max(cond))
  # 
  # return(max_cond)
  # 
  
  # number of COVID-19 positive individuals in the database
  pos_status_data <- tibble()
  
  # loop to guarantee we assign some COVID-19 positive individuals in the data
  # simulate 1% of individuals in Y as COVID-19 positive
  while(nrow(pos_status_data) == 0) {
  
    # add covid status to location data
    status_data <- loc_data %>% 
      group_by(patient_id) %>%
      mutate(status = sample(c(0, 1), size = 1, prob = c(.99, .01))) %>%
      ungroup()
    
    # data corresponding to covid positive individuals (while loop condition just needs the number of rows > 0)
    pos_status_data <- status_data %>%
      filter(status == 1)

  }

  ## (2)
  
  # randomly sample 'traj_length' points from each trajectory
  # split into list - each element is a sampled trajectory (patient_id is dropped)
  sampled_data <- status_data %>%
    group_by(patient_id) %>%
    slice_sample(n = traj_length) %>%
    ungroup()
  

  ## (3)
  ei <- sampled_data %>%
    filter(status == 1) %>%
    select(-status) %>%
    group_by(patient_id) %>%
    slice_sample(n=1) %>%
    group_split(.keep=FALSE)

  ## (4)

  # take 'original' data, split on a per-trajectory basis, and remove
  # unique trajectory/status combinations

  sampled_data <- sampled_data %>%
    group_by(patient_id) %>%
    group_split()



  is_matched <- lapply(ei, function(y) sapply(sampled_data, function(x) nrow(match_df(y, x, on=c("latitude", "longitude"))) > 0))

  max_increase <- max(sapply(is_matched, function(x) mean(sapply(sampled_data[x], function(y) unique(y$status))) - prior))
  
  max_increase <- tibble("max_increase"=max_increase)
  
}
```

Function to perform simulation one hundred times each for one to five external information tuples based on an input dataset and a specified value of d (num_dec).
```{r}
inf_calc <- function(location_data, num_dec, nits = 100, sample_sizes = 1:5, pvar_names = c('one', 'two', 'three', 'four', 'five')) {
  
  loc_data <- location_data %>%
    mutate_at(.vars = c('latitude', 'longitude'), round2, n = num_dec)
  
  props <- map_dfc(sample_sizes, function(y) map_dfr(1:nits, function(x) inference_simulation(loc_data, y)))
  
  colnames(props) <- pvar_names
  
  props_g <- props %>%
    gather(key = 'num_points', value = 'inf_prop') %>%
    mutate(specificity = num_dec)
  
  return(props_g)
  
}
```


Inference analysis for full data.
```{r fig.width=6, fig.height=6}
# inf_results <- inf_calc(full, 2) %>%
#   bind_rows(inf_calc(full, 1)) %>%
#   bind_rows(inf_calc(full, 0))
# 
# save(inf_results, file = '../Data/inf_results_trajectories_full.RData')

load('../Data/inf_results_trajectories_full.RData')

i_p <- inf_results %>%
  mutate(specificity = factor(specificity, levels = c('0', '1', '2')),
         num_points = factor(num_points, levels = c('one', 'two', 'three', 'four', 'five'))) %>%
  ggplot(aes(x = specificity, y = inf_prop, fill = factor(num_points))) +
  geom_boxplot(position = position_dodge(width = 0.5)) +
  labs(x = 'Number of decimals d',
       y = 'Max Inference Condition Across Positive Individuals',
       fill = '# Tuples\n',
       title = 'Full Data') +
  scale_fill_discrete(labels = c('One', 'Two', 'Three', 'Four', 'Five'),
                      guide = guide_legend(reverse = TRUE)) +
  theme(legend.position = c(0.025, 0.95),
        legend.justification = c('left', 'top'),
        legend.box.just = 'left') +
  scale_y_continuous(limits = c(-0.01, 1.0), breaks = seq(0, 1.0, by = 0.25))

addSmallLegend(i_p)
```

Inference analysis for Seoul data.
```{r fig.width=6, fig.height=6}
# inf_results_s <- inf_calc(s_full, 2) %>%
#   bind_rows(inf_calc(s_full, 1)) %>%
#   bind_rows(inf_calc(s_full, 0))
# 
# save(inf_results_s, file = '../Data/inf_results_trajectories_seoul.RData')

load(file = '../Data/inf_results_trajectories_seoul.RData')

i_p_s <- inf_results_s %>%
  mutate(specificity = factor(specificity, levels = c('0', '1', '2')),
         num_points = factor(num_points, levels = c('one', 'two', 'three', 'four', 'five'))) %>%
  ggplot(aes(x = specificity, y = inf_prop, fill = factor(num_points))) +
  geom_boxplot(position = position_dodge(width = 0.5)) +
  labs(x = 'Number of decimals d',
       y = '',
       fill = '# Tuples\n',
       title = 'Seoul Only') +
  scale_fill_discrete(labels = c('One', 'Two', 'Three', 'Four', 'Five'),
                      guide = guide_legend(reverse = TRUE)) +
  theme(legend.position = c(0.025, 0.95),
        legend.justification = c('left', 'top'),
        legend.box.just = 'left',
        text = element_text(size = 12.5),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  scale_y_continuous(limits = c(-0.01, 1.0), breaks = seq(0, 1.0, by = 0.25))

addSmallLegend(i_p_s)
```

```{r fig.width=11, fig.height=7}
addSmallLegend(i_p) | addSmallLegend(i_p_s)
```

### Inference Analysis for Microaggregation

Steps:

(1) Assign 1% of individuals a positive covid-19 status.
(2) Remove longitudinal links (conceptual)
(3) Simulate subsets of locations that are external information to the adversary
(4) Perform matching for adversary - link each external information tuple to
its nearest-neighbor microaggregated tuple in the protected data. Posterior
probability is the proportion of matched points with COVID-19 status = 1.

(1) Assign 1% of individuals a positive covid-19 status.
```{r}
# # number of COVID-19 positive individuals in the database
# pos_status_data <- tibble()
#   
# # loop to guarantee we assign some COVID-19 positive individuals in the data
# # simulate 1% of individuals in Y as COVID-19 positive
# while(nrow(pos_status_data) == 0) {
#   
#   # add covid status to location data
#   status_data <- full %>% 
#     group_by(patient_id) %>%
#     mutate(status = sample(c(0, 1), size = 1, prob = c(.99, .01))) %>%
#     ungroup()
#     
#   # data corresponding to covid positive individuals (while loop condition just needs the number of rows > 0)
#   pos_status_data <- status_data %>%
#     filter(status == 1)
# 
# }
```


(2) Remove longitudinal links (conceptual)
(3) Sample a location tuple from the trajectory of each covid-positive person.
```{r}
# status <- status_data %>% pull(status)
# 
# ei <- status_data %>%
#   filter(status==1) %>%
#   group_by(patient_id) %>%
#   slice_sample(n=1) %>%
#   ungroup() %>%
#   select(latitude, longitude)
```

```{r}
# protected_data <- ma_data_sets[[1]]
# ids <- full$patient_id
# 
# # add status to locations for probability computation later
# ds <- protected_data %>% mutate(status=status, pid=ids)
# 
# ds %>%
#   group_by(latitude, longitude, status) %>%
#   mutate(n=n()) %>%
#   filter(n >= 2)
```


For each protected data set, compute the posterior probability of inference for each EI tuple.
```{r}
max_post_prob <- function(protected_data, ids, ei_tuples, disease_status, prior){
  
  # add status to locations for probability computation later
  ds <- protected_data %>% mutate(status=disease_status, pid=ids)
  
  # remove unique records
  ds <- ds %>%
    group_by(latitude, longitude, status) %>%
    mutate(n=n()) %>%
    ungroup() %>%
    filter(n >= 2) %>%
    select(latitude, longitude, status, pid)
  
  # get indexes of the nearest neighbor to each point
  neighbor_indexes <- get.knnx(data=ds[,c('latitude', 'longitude')], query=ei_tuples[,c('latitude', 'longitude')], k=1, algorithm="kd_tree")$nn.index
  
  # calculate posterior probability
  posterior_probs <- c()
  proportion_target <- c()
  for(i in seq_along(neighbor_indexes)){
    matched <- ds %>% slice(neighbor_indexes[i])
  
    matched_k <- ds %>%
      filter(latitude==matched$latitude, longitude==matched$longitude)

    posterior_probs <- c(posterior_probs, mean(matched_k$status)-prior)
    
    proportion_target <- c(proportion_target, mean(matched_k$pid == (ei_tuples$patient_id)[i]))
      
  }
  
  return(tibble(Inf_condition=max(posterior_probs), prop_target=mean(proportion_target)))
  
}

# lapply(ma_data_sets, function(x) max_post_prob(x, ei, status))

# GOAL: the average of the LHS across simulated EI sets for each protected
# data set.
```


Combine everything. Compute the distribution of the maximum of the LHS of the inference condition.
```{r}
ma_inference_simulation <- function(original_data, protected_data_sets, kvals, prior=0.01){
  
  ### (1) Assign 1% of individuals a positive covid-19 status.
  
  # number of COVID-19 positive individuals in the database
  pos_status_data <- tibble()
  
  # loop to guarantee we assign some COVID-19 positive individuals in the data
  # simulate 1% of individuals in Y as COVID-19 positive
  while(nrow(pos_status_data) == 0) {
  
    # add covid status to location data
    status_data <- original_data %>% 
      group_by(patient_id) %>%
      mutate(status = sample(c(0, 1), size = 1, prob = c(.99, .01))) %>%
      ungroup()
    
    # data corresponding to covid positive individuals (while loop condition just needs the number of rows > 0)
    pos_status_data <- status_data %>%
      filter(status == 1)

  }
  
  status <- status_data %>% pull(status)

  ei <- status_data %>%
    filter(status==1) %>%
    group_by(patient_id) %>%
    slice_sample(n=1) %>%
    ungroup()

  res <- map_dfr(protected_data_sets, function(x) max_post_prob(x, original_data$patient_id, ei, status, prior))
  
  res$k <- kvals
  
  return(res)

}
```

Perform the simulation 100 times.
```{r}
# full_ma_inf <- map_dfr(1:100, function(x) ma_inference_simulation(full, ma_data_sets, ma_k))
# s_ma_inf <- map_dfr(1:100, function(x) ma_inference_simulation(s_full, mas_data_sets, ma_k))
# 
# save(full_ma_inf, file = '../Data/inf_results_ma_full.RData')
# save(s_ma_inf, file = '../Data/inf_results_ma_s.RData')

load('../Data/inf_results_ma_full.RData')
load('../Data/inf_results_ma_s.RData')
```

```{r}
full_ma_inf <- full_ma_inf %>%
  group_by(k) %>%
  mutate(avg_prop_target=mean(prop_target))

s_ma_inf <- s_ma_inf %>%
  group_by(k) %>%
  mutate(avg_prop_target=mean(prop_target))
```


Plot
```{r fig.width = 10, fig.height = 8}
full_ma_inf_plot <- full_ma_inf %>%
  ggplot(aes(x=factor(k, levels=ma_k), y=Inf_condition)) +
  geom_boxplot() +
  geom_point(aes(x=factor(k, levels=ma_k), y=avg_prop_target), color='red', shape=8, size=2) +
  labs(x = 'k',
       y = 'Max Inference Condition Across Positive Individuals',,
       title = 'Full Data') +
  scale_x_discrete(limits=rev) +
  scale_y_continuous(limits = c(-0.01, 1.0), breaks = seq(0, 1.0, by = 0.25))
  

s_ma_inf_plot <- s_ma_inf %>%
  ggplot(aes(x=factor(k, levels=ma_k), y=Inf_condition)) +
  geom_boxplot() +
  geom_point(aes(x=factor(k, levels=ma_k), y=avg_prop_target), color='red', shape=8, size=2) +
  labs(x = 'k',
       y = '',
       title = 'Seoul Only') +
  scale_x_discrete(limits=rev) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  scale_y_continuous(limits = c(-0.01, 1.0), breaks = seq(0, 1.0, by = 0.25))

(addSmallLegend(i_p) + addSmallLegend(i_p_s)) / (full_ma_inf_plot + s_ma_inf_plot)
```





<!-- ### Inference analysis for Counts. -->

<!-- Function to simulate an adversary having access to one location tuple corresponding to individual i and linking that tuple to the count for that tuple in an aggregated counts database to determine the COVID-19 status of that individual. -->
<!-- ```{r} -->
<!-- inf_counts <- function(location_data, num_dec, prior = 0.01) { -->

<!--   # dataframe for COVID-19 positive individuals -->
<!--   pos_status_data <- tibble() -->

<!--   # loop to guarantee we assign some COVID-19 positive individuals in the data -->
<!--   # simulate 1% of individuals in Y as COVID-19 positive -->
<!--   while(nrow(pos_status_data) == 0) { -->

<!--     # add covid status to location data -->
<!--     status_data <- location_data %>%  -->
<!--       group_by(patient_id) %>% -->
<!--       mutate(status = sample(c(0, 1), size = 1, prob = c(.99, .01))) %>% -->
<!--       ungroup() -->

<!--     # data corresponding to covid positive individuals (while loop condition just needs the number of rows > 0) -->
<!--     pos_status_data <- status_data %>% -->
<!--       filter(status == 1) -->

<!--   } -->

<!--   # round data to specified value of d -->
<!--   rounded_status_data <- status_data %>% -->
<!--     mutate_at(.vars = c('latitude', 'longitude'), round2, n = num_dec) -->

<!--   # get all distinct location tuples for all COVID-19 positive individuals -->
<!--   covid_locs <- rounded_status_data %>% -->
<!--     filter(status == 1) %>% -->
<!--     distinct(latitude, longitude) -->

<!--   # get the regions from the rounded data that match the regions visited by the COVID-19 positive individuals -->
<!--   # for each distinct region, calculate the LHS of the inference condition (the proportion of individuals with tuples in the region - prior -->
<!--   count_props <- match_df(rounded_status_data, covid_locs, on = c('latitude', 'longitude')) %>% -->
<!--     group_by(latitude, longitude) %>% -->
<!--     summarize(inc_inf = mean(status) - prior, .groups = 'drop') -->

<!--   # calculate the average of the LHS of the inference condition across the regions visited by each individual -->
<!--   ind_avg_count_props <- pos_status_data %>% -->
<!--     mutate_at(.vars = c('latitude', 'longitude'), round2, n = num_dec) %>% # round data corresponding to positive individuals -->
<!--     distinct(patient_id, latitude, longitude) %>% # take the distinct regions visited by each positive individual -->
<!--     left_join(count_props, by = c('latitude', 'longitude')) %>% # merge the LHS of inference condition calculated for each region in `count_props` -->
<!--     group_by(patient_id) %>% -->
<!--     summarize(avg_inc_inf = mean(inc_inf), .groups = 'drop') %>% # calculate the average across regions for each individual -->
<!--     select(avg_inc_inf) %>% -->
<!--     mutate(specificity = num_dec) # add variable indicating the number of decimals -->

<!--   return(ind_avg_count_props) -->

<!-- } -->
<!-- ``` -->

<!-- Perform simulation 100 times for Full data for d = 0,1,2,3,4,5 and plot results. -->
<!-- ```{r} -->
<!-- # inf_count_dists <- map_dfr(0:5, function(y) map_dfr(1:100, function(x) inf_counts(full, y))) %>% -->
<!-- #   mutate(specificity = factor(specificity, levels = c('0', '1', '2', '3', '4', '5'))) -->

<!-- # save(inf_count_dists, file = '../Data/counts_inference_full.RData') -->
<!-- load('../Data/counts_inference_full.RData') -->

<!-- count_inf_full <- inf_count_dists %>% -->
<!--   ggplot(aes(x = specificity, y = avg_inc_inf)) + -->
<!--   geom_boxplot() + -->
<!--   labs(x = 'Number of decimals', -->
<!--        y = 'Individual Level Average of LHS of Condition', -->
<!--        title = 'Full Data') + -->
<!--   scale_y_continuous(limits = c(-.05, 1), breaks = seq(0, 1, by = .25)) + -->
<!--   theme(text = element_text(size = 12.5)) -->
<!-- ``` -->

<!-- Repeat for Seoul. -->
<!-- ```{r} -->
<!-- # inf_count_dists_s <- map_dfr(0:5, function(y) map_dfr(1:100, function(x) inf_counts(s_full, y))) %>% -->
<!-- #   mutate(specificity = factor(specificity, levels = c('0', '1', '2', '3', '4', '5'))) -->
<!-- #  -->
<!-- # save(inf_count_dists_s, file = '../Data/counts_inference_seoul.RData') -->
<!-- load('../Data/counts_inference_seoul.RData') -->

<!-- count_inf_s <- inf_count_dists_s %>% -->
<!--   ggplot(aes(x = specificity, y = avg_inc_inf)) + -->
<!--   geom_boxplot() + -->
<!--   labs(y = '', -->
<!--        x = 'Number of decimals', -->
<!--        title = 'Seoul Only') + -->
<!--   scale_y_continuous(limits = c(-.05, 1), breaks = seq(0, 1, by = .25)) + -->
<!--   theme(text = element_text(size = 12.5), -->
<!--         axis.text.y = element_blank(), -->
<!--         axis.ticks.y = element_blank()) -->
<!-- ``` -->

<!-- ```{r fig.width=11, fig.height=7} -->
<!-- count_inf_full | count_inf_s -->
<!-- ``` -->

<!-- ```{r} -->
<!-- inf_count_dists_s %>% -->
<!--   filter(specificity == 1) %>% -->
<!--   summarize(m = max(avg_inc_inf)) -->
<!-- ``` -->

